To run this project: python main.py

Most Python scripts run on a single core, but PCs have multiple cores. It's
possible to write Python scripts that divide up the work and have each of
those divisions run simultaneously on separate cores.  This project
demonstrates three different ways of doing just that.  Assume you have a list
(an iterable) of say 10 files and you want to process each one of them with a
function called "worker".

Normally it would be done like this: for ii in len(fList):
                                         worker(fList[ii])

CONCEPTUALLY: By using multiple, say 2, cores it can be done like this:

On core1:                         simultaneously on core2:
--------------------------------  ------------------------------------------
for ii in range(0,len(fList)/2):  for ii in range(len(fList)/2, len(fList)):
    worker(fList[ii])                 worker(fList[ii])

IN PRACTICE: You take your (flat) iterable and divide it into a number of
"chunks".  The number of chunks you divide it up into is the number of cores
you want to run on.  The more cores the faster everything gets done.  My i7
has 28 cores!!

Here's an example of chunking a (flat) iterable into three chunks.

chunkify([ a, b, c, d, e, f, g, h ], 3) =
[ [a, b, c], [d, e, f], [g, h] ]

When you run this main.py the iterable will be handled the "normal" way and
all multi-core ways that I've called: brute-force, executor and pool.  Each
way has pros and cons.  Each of the 4 ways give the same answer. Whew.

[LATENT BUG FIXED]
If your flat iterable has 10 things and you run on 10 cores (numProcs), then
each core will work on 10/10=1 thing. If numProcs=5, then each core will work
on 10/5=2 things.  If numProcs=20, it won't crash but, in fact only 10 cores
will be used (you run out of data after 10 cores - each processing 1 item in
the iterable).

I've tried to write everything such that you should only have to change 2
things to port to your use-case: (1) the function "worker" in file worker.py
and variable "flatIterable" in file main.py.

Each of the multi-core ways have (basically) the same execution time so 
that's not one of their distinguishing characteristics.

A summary/comparison of the three multi-core methods is provided below.
During development I recommend getting your use case running via all methods
and verify that each gives the same answer.  Then, if I had to choose which 
one to go to production with, I'd go with executor (see bottom paragraph).


                                          bruteForce  executor  pool
               ~Number of lines of code:  18          16        11
            Can return results directly:  NO (1)      YES       YES
          Can accept multiple arguments:  YES         YES       NO (2)
    Can give each process a unique name:  YES         YES       NO (3)

(1) The worker function's result (returned value, if any) cannot be returned
directly with the bruteForce method.  So, what the worker function does is
place anything it wants to return in a queue.  The q is passed from the top
process (main) all the way down to the worker(s) and then back up.  Be aware 
that if 10 processes are running then they will/may be writing to the q 
simultaneously! That's okay, the q's are thread/process safe.

(2) The pool method cannot accept multiple arguments and this presented a
problem when trying to pass the queue to the worker function. This problem
was solved through the use of python's functools.partial method.

(3) The pool method cannot assign a unique name to each of the (identical) 
instances of the process (worker).  Printed result include with the name of
the process that provided that particular result.  So, the pool process names
are all identical even though they are probably different processes.

Three example use cases are provided; (1) calculating the number of prime
numbers in a range and (2) simulated processing a list of files and (3) 
processing a list of .csv files with the pandas import.

Examples (2) and (3) are easier to understand than example (1). Example 1
doesn't really start out with a completely flat list but rather a list of
lists (the start and stop values of the sub-range).  This complication is 
handled in the worker function itself.

The worker functions for examples (1,2) are contained in file worker.py.
The worker function for example (3) is contained county_summary.py.

When main.py is run all .py files in this project are used, except one.
When main.py is run file county_summary_stand_alone.py is not used.

[EXAMPLE OF MOVING TO PRODUCTION]
county_summary_stand_alone.py may be run directly via command:
   python county_summary_stand_alone.py

When county_summary_stand_alone.py is run directly no other file in this
project is accessed - it is truely, stand alone.

county_summary_stand_alone.py demonstrates using just the executor method for
implementing multicore processing.  Since this method, as noted above,
supports returning results directly the queue parameter and it's associated
processing can be (and is) removed.
